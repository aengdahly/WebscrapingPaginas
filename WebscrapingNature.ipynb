{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WebscrapingNature.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aengdahly/WebscrapingPaginas/blob/main/WebscrapingNature.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-jKPd4w43U-"
      },
      "source": [
        "# Webscraping para Revista Nature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_-xw6H95WI9"
      },
      "source": [
        "## Paquetes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPcqEIhn42C5"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "import regex\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ESmWaoJ58UU"
      },
      "source": [
        "## Completar la frase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P9I0Ahq5UsD"
      },
      "source": [
        "frase='marine litter'\n",
        "url_base='https://www.nature.com/search?q={}&order=relevance&article_type=research'.format(frase)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYZIc6Ys5TZr"
      },
      "source": [
        "page= requests.get(url_base, verify=False)\n",
        "html = BeautifulSoup(page.content, 'html')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGV1aaKuTlnM"
      },
      "source": [
        "## Funciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALQyzLuUTohm"
      },
      "source": [
        "def EliminarSaltosLinea(oracion):\n",
        "    string = regex.sub(r'[\\n\\r\\t\\xa0\\xad]',' ', oracion)\n",
        "    return(''.join(c for c in string))\n",
        "\n",
        "def EliminarEspaciosExtras(oracion):\n",
        "    string = regex.sub(r'\\s{2,}',' ', oracion)\n",
        "    return(''.join(c for c in string))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7IIdsUsGXvW"
      },
      "source": [
        "## Obtener la última página."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeoEG4df7RIm"
      },
      "source": [
        "paginacion=html.findAll('li',{'class': 'inline-group-item inline-group-middle'})\n",
        "ult_page=int(paginacion[-2].text.replace('\\n\\npage',''))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rly51NcH8rHn"
      },
      "source": [
        "datos = pd.DataFrame(columns=['Titulo','Pagina','Abstract','Archivo'])\n",
        "\n",
        "for i in range(1,ult_page+1) : #\n",
        "    # por cada pàgina\n",
        "    if i==1:\n",
        "        url=url_base\n",
        "    else:\n",
        "        url=url_base+'&page={}'.format(i)\n",
        "\n",
        "    page= requests.get(url, verify=False)\n",
        "    html = BeautifulSoup(page.content, 'html')\n",
        "    titulos=html.findAll('h2',{'class': 'h3 extra-tight-line-height'})\n",
        "    links=[z.find('a',{'href': True})['href'] for z in titulos]\n",
        "  \n",
        "\n",
        "    for j in links:\n",
        "      direccion='https://www.nature.com{}'.format(j)\n",
        "      pagina_articulo=requests.get(direccion, verify=False)\n",
        "      html2= BeautifulSoup(pagina_articulo.content, 'html')\n",
        "\n",
        "      titulo=html2.find('h1',{'class':'c-article-title'}).text\n",
        "      p=html2.find('div',{'class':'c-article-section__content','id':'Abs1-content'})\n",
        "      if (p!=None):\n",
        "        abstract=EliminarEspaciosExtras(EliminarSaltosLinea(p.text))\n",
        "      else:\n",
        "        abstract='No tiene Abstract como html'  \n",
        "      archivo=direccion+'.pdf'\n",
        "      datos = datos.append({'Titulo':titulo,'Pagina':direccion,'Abstract':abstract,'Archivo': archivo}, ignore_index=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgrIdVE3KK-5"
      },
      "source": [
        "## Guardar archivo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWwy3ictI0Gz"
      },
      "source": [
        "datos.to_csv(frase+'.csv',index=False, sep='|')\n",
        "files.download(frase+'.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}