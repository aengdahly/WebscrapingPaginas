{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WebscrapingImagenGoogle.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aengdahly/WebscrapingPaginas/blob/main/WebscrapingImagenGoogle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNJYRcxSAWcU"
      },
      "source": [
        "# Script para descargar imágenes de Google.\n",
        "\n",
        "Código principal y créditos para el webscraping de las imágenes de google  https://github.com/Ladvien/deep_arcane \n",
        "\n",
        "Al código anterior se le adicionó:\n",
        "1. Guardar las imágenes en google drive.\n",
        "2. Utilizar selenium en colab.\n",
        "3. Guardar en un excel el nombre del archivo, url y la breve descripción de la imagen\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rb8n3jLBb52"
      },
      "source": [
        "## Librerías."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlmNvaewAdK2"
      },
      "source": [
        "\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "import urllib\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "import io\n",
        "import hashlib\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "import signal\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium\n",
        "\n",
        "from selenium import webdriver\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "wd = webdriver.Chrome('chromedriver',options=options)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl6WZIkTDOF3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPGeESITAraB"
      },
      "source": [
        "ruta='/content/gdrive/MyDrive/picture/'\n",
        "search_terms =['\"zorzal\"']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkHMcYAnA6hs"
      },
      "source": [
        "number_of_images = 400\n",
        "GET_IMAGE_TIMEOUT = 5\n",
        "SLEEP_BETWEEN_INTERACTIONS = 0.1\n",
        "SLEEP_BEFORE_MORE = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8z6n0JZCJax"
      },
      "source": [
        "##########\n",
        "# Scrap\n",
        "##########\n",
        "\n",
        "\n",
        "wd.get(\"https://google.com\")\n",
        "\n",
        "\n",
        "# Credit:\n",
        "# https://stackoverflow.com/a/22348885\n",
        "class timeout:\n",
        "    def __init__(self, seconds=1, error_message=\"Timeout\"):\n",
        "        self.seconds = seconds\n",
        "        self.error_message = error_message\n",
        "\n",
        "    def handle_timeout(self, signum, frame):\n",
        "        raise TimeoutError(self.error_message)\n",
        "\n",
        "    def __enter__(self):\n",
        "        signal.signal(signal.SIGALRM, self.handle_timeout)\n",
        "        signal.alarm(self.seconds)\n",
        "\n",
        "    def __exit__(self, type, value, traceback):\n",
        "        signal.alarm(0)\n",
        "\n",
        "\n",
        "def fetch_image_urls(\n",
        "    query: str,\n",
        "    max_links_to_fetch: int,\n",
        "    wd: webdriver,\n",
        "    sleep_between_interactions: int = 1,\n",
        "):\n",
        "    def scroll_to_end(wd):\n",
        "        wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        time.sleep(sleep_between_interactions)\n",
        "\n",
        "    # build the google query\n",
        "    search_url = \"https://www.google.com/search?safe=off&site=&tbm=isch&source=hp&q={q}&oq={q}&gs_l=img\"\n",
        "\n",
        "    # load the page\n",
        "    wd.get(search_url.format(q=query))\n",
        "\n",
        "    image_urls = []\n",
        "    image_texts=[]\n",
        "    link_page=[]\n",
        "    link_title=[]\n",
        "    image_count = 0\n",
        "    results_start = 0\n",
        "    while image_count < max_links_to_fetch:\n",
        "        scroll_to_end(wd)\n",
        "\n",
        "        # get all image thumbnail results\n",
        "        thumbnail_results = wd.find_elements_by_css_selector(\"img.rg_i.Q4LuWd\")\n",
        "        number_results = len(thumbnail_results)\n",
        "\n",
        "        print(\n",
        "            f\"Found: {number_results} search results. Extracting links from {results_start}:{number_results}\"\n",
        "        )\n",
        "\n",
        "        for img in thumbnail_results[results_start:number_results]:\n",
        "            # try to click every thumbnail such that we can get the real image behind it\n",
        "            try:\n",
        "                img.click()\n",
        "                time.sleep(sleep_between_interactions)\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "            # extract image urls\n",
        "            actual_images = wd.find_elements_by_css_selector(\"img.n3VNCb\")\n",
        "            for actual_image in actual_images:\n",
        "                if actual_image.get_attribute(\n",
        "                    \"src\"\n",
        "                ) and \"http\" in actual_image.get_attribute(\"src\"):\n",
        "                    image_urls.append(actual_image.get_attribute(\"src\"))\n",
        "                    if actual_image.get_attribute(\"alt\"):\n",
        "                      image_texts.append(actual_image.get_attribute(\"alt\")) \n",
        "                    else: image_texts.append('Sin texto')\n",
        "\n",
        "            image_count = len(image_urls)\n",
        "\n",
        "            actual_link =wd.find_elements_by_css_selector(\"a.Beeb4e\")\n",
        "            for link in actual_link:\n",
        "                if link.get_attribute(\n",
        "                    \"href\"\n",
        "                ) and \"http\" in link.get_attribute(\"href\"):\n",
        "                    link_page.append(link.get_attribute(\"href\"))\n",
        "                    if link.get_attribute(\"title\"):\n",
        "                      link_title.append(link.get_attribute(\"title\")) \n",
        "                    else: link_title.append('Sin texto')\n",
        "\n",
        "\n",
        "            if len(image_urls) >= max_links_to_fetch:\n",
        "                print(f\"Found: {len(image_urls)} image links, done!\")\n",
        "                break\n",
        "        else:\n",
        "            print(\"Found:\", len(image_urls), \"image links, looking for more ...\")\n",
        "            time.sleep(SLEEP_BEFORE_MORE)\n",
        "\n",
        "            not_what_you_want_button = \"\"\n",
        "            try:\n",
        "                not_what_you_want_button = wd.find_element_by_css_selector(\".r0zKGf\")\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            # If there are no more images return.\n",
        "            if not_what_you_want_button:\n",
        "                print(\"No more images available.\")\n",
        "                return image_urls, image_texts\n",
        "\n",
        "            load_more_button = wd.find_element_by_css_selector(\".mye4qd\")\n",
        "            if load_more_button and not not_what_you_want_button:\n",
        "                wd.execute_script(\"document.querySelector('.mye4qd').click();\")\n",
        "\n",
        "        # move the result startpoint further down\n",
        "        results_start = len(thumbnail_results)\n",
        "\n",
        "    return image_urls,image_texts, link_page, link_title\n",
        "\n",
        "\n",
        "def persist_image(folder_path: str, url: str):\n",
        "    try:\n",
        "        print(\"Getting image\")\n",
        "        with timeout(GET_IMAGE_TIMEOUT):\n",
        "            image_content = requests.get(url).content\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR - Could not download {url} - {e}\")\n",
        "\n",
        "    try:\n",
        "        image_file = io.BytesIO(image_content)\n",
        "        image = Image.open(image_file).convert(\"RGB\")\n",
        "        file_path = os.path.join(\n",
        "            folder_path, hashlib.sha1(image_content).hexdigest()[:10] + \".jpg\"\n",
        "        )\n",
        "        with open(file_path, \"wb\") as f:\n",
        "            image.save(f, \"JPEG\", quality=100)\n",
        "        print(f\"SUCCESS - saved {url} - as {file_path}\")\n",
        "        info=f\"SUCCESS - saved {url} - as {file_path}\"\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR - Could not save {url} - {e}\")\n",
        "        info=f\"ERROR - Could not save {url} - {e}\"\n",
        "    return(info)\n",
        "\n",
        "\n",
        "def search_and_download(search_term: str, target_path=\"./images\", number_images=5):\n",
        "    target_folder = os.path.join(target_path, \"_\".join(search_term.lower().split(\" \")))\n",
        "\n",
        "    if not os.path.exists(target_folder):\n",
        "        os.makedirs(target_folder)\n",
        "\n",
        "    with webdriver.Chrome('chromedriver',options=options) as wd:\n",
        "        res,texts, link, title = fetch_image_urls(\n",
        "            search_term,\n",
        "            number_images,\n",
        "            wd=wd,\n",
        "            sleep_between_interactions=SLEEP_BETWEEN_INTERACTIONS,\n",
        "        )\n",
        "\n",
        "        rest=[]\n",
        "        if res is not None:\n",
        "            for elem in res:\n",
        "                rest.append(persist_image(target_folder, elem))\n",
        "        else:\n",
        "            print(f\"Failed to return links for term: {search_term}\")\n",
        "    return res,texts,rest, link, title\n",
        "\n",
        "\n",
        "for term in search_terms:\n",
        "    info=search_and_download(term, ruta+term, number_of_images)\n",
        "    archivo=pd.DataFrame({'Pag_imagen':info[0],'Titulo':info[1],'Descarga':info[2], 'Archivo':[i.split('/')[-1] for i in info[2]]})\n",
        "    archivo2=pd.DataFrame({'Link':info[3],'Titulo':info[4]})\n",
        "    archivo.to_excel(ruta+term+'.xlsx')\n",
        "    archivo2.to_excel(ruta+term+'_2.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hunSM-ulpYMB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}